{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "-------------\n",
    "\n",
    "In this document, we are going to explain about the A la Carte Embeddings (ALC) but, before introducing it, we will cover some background regarding Natural Language Processing (NLP) basics and Word Embeddings.\n",
    "\n",
    "We will start by exploring the basics of text preprocessing and representation. Then, we will introduce the concept of Word Embeddings and Finally start explaining ALC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Representation:\n",
    "--------------------\n",
    "\n",
    "Text representation is the process of transforming text into a structured and numerical representation that can be understood by NLP or Machine Learning algorithms.\n",
    "\n",
    "The **goal** of text representation is to **transform** text to **extract knowledge** and **meaning** about the data.\n",
    "\n",
    "This is usually done by converting text into a vector of numbers that can be understood by the algorithms.\n",
    "\n",
    "After the text is converted into a vector of numbers, we can obtain better understand of the data by applying different techniques like measuring the distance between the vectors.\n",
    "\n",
    "The **most common** text representation techniques are:\n",
    "\n",
    "- **Bag of Words (BoW)**: This technique represents text as a bag of words, where each word is represented by a number. The number represents the frequency of the word in the text. This technique is very simple and easy to implement, but it has some drawbacks. It does not consider the order of the words, and it does not consider the context of the words.\n",
    "\n",
    "- **TF-IDF**: This technique is similar to BoW, but it also considers the importance of the word in the text. The importance of the word is calculated by multiplying the frequency of the word in the text by the inverse document frequency. The inverse document frequency is calculated by dividing the number of documents by the number of documents that contain the word. This technique is also simple and easy to implement, but it also has some drawbacks. It does not consider the order of the words, and it does not consider the context of the words.\n",
    "\n",
    "- **Word Embeddings**: This technique represents words as vectors of numbers. The vectors are calculated by considering the context of the words. This technique is more complex and harder to implement, but it has some advantages. It considers the order of the words, and it considers the context of the words.\n",
    "\n",
    "**`In this document, we will focus on Word Embeddings.`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP preprocessing techniques:\n",
    "-----------------------------\n",
    "\n",
    "Before applying any text representation technique, we need to preprocess the text. The preprocessing techniques are used to clean the text and to prepare it for the text representation techniques.\n",
    "\n",
    "The **most common** preprocessing techniques are:\n",
    "\n",
    "- **Tokenization**: This technique is used to split the text into tokens. The tokens can be words, sentences, or paragraphs. The most common tokenization technique is word tokenization, where the text is split into words.\n",
    "    - `Example`: `\"I like to eat apples and bananas.\"` -> `[\"I\", \"like\", \"to\", \"eat\", \"apples\", \"and\", \"bananas\"]`\n",
    "\n",
    "- **Normalization**: This technique is used to convert the text to lowercase, remove punctuation, and remove numbers.\n",
    "    - `Example`: `\"I like to eat apples and bananas.\"` -> `\"i like to eat apples and bananas\"`\n",
    "\n",
    "- **Stop words removal**: This technique is used to remove stop words from the text. Stop words are words that are very common in the language, and they do not add any value to the text.\n",
    "    - `Example`:`\"the\"`, `\"a\"`, `\"an\"`, `\"in\"`, `\"on\"`, `\"at\"`.\n",
    "\n",
    "- **Stemming**: This technique is used to reduce words to their root form. The most common stemming technique is Porter Stemming, where words are reduced to their root form by removing the suffixes.\n",
    "    - `Example`: `\"running\" -> \"run\"`, `\"eats\" -> \"eat\"`, `\"eating\" -> \"eat\"`, `\"a -> \"a\"`\n",
    "\n",
    "- **Lemmatization**: This technique is used to reduce words to their root form. The most common lemmatization technique is WordNet Lemmatization, where words are reduced to their root form by using a dictionary.\n",
    "    - `Example`: `\"running\" -> \"run\"`, `\"eats\" -> \"eat\"`, `\"eating\" -> \"eat\"`, `\"a -> \"a\"`\n",
    "\n",
    "**Note**: The difference between stemming and lemmatization is that stemming uses a set of rules to reduce words to their root form, while lemmatization uses a dictionary to reduce words to their root form. The result of stemming is not always a valid word, while the result of lemmatization is always a valid word. This also means that stemming is faster than lemmatization, but it is less accurate.\n",
    "\n",
    "There are other preprocessing techniques which despite being common, we wont cover them in this document for the sake of simplicity but we can mention them: Part of Speech (POS) tagging, Named Entity Recognition (NER).\n",
    "\n",
    "Preprocessing also involves making pipelines where we can combine different preprocessing techniques to obtain better results.\n",
    "Example:\n",
    "\n",
    "```\n",
    "Preprocessing pipeline: Tokenization -> Normalization -> Stop words removal -> Stemming\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"Tokenize text by splitting on whitespace\n",
    "    Args:\n",
    "        text (str): The string to tokenize\n",
    "    Returns:\n",
    "        list: The tokenized list of strings\n",
    "    \"\"\"\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "def normalize(tokens):\n",
    "    \"\"\"Normalize tokens by lowercasing them and removing punctuation\n",
    "    Args:\n",
    "        tokens (list): List of tokenized words\n",
    "    Returns:\n",
    "        list: List of normalized words\n",
    "    \"\"\"\n",
    "    punctuation = ['.', ',', '!', '?', ';', ':']\n",
    "    normalized_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        # Convert to lowercase\n",
    "        normalized_token = token.lower()\n",
    "\n",
    "        # Remove punctuation\n",
    "        normalized_token = ''.join(char for char in normalized_token if char not in punctuation)\n",
    "        normalized_tokens.append(normalized_token)\n",
    "    \n",
    "    return normalized_tokens\n",
    "\n",
    "def stem(tokens):\n",
    "    \"\"\"Stem tokens using Porter stemmer\n",
    "    Args:\n",
    "        tokens (list): List of tokenized words\n",
    "    Returns:\n",
    "        list: List of stemmed words\n",
    "    \"\"\"\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        stemmed_tokens.append(token[:-1])\n",
    "    return stemmed_tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    \"\"\"Remove stop words from list of tokenized words\n",
    "    Args:\n",
    "        tokens (list): List of tokenized words\n",
    "    Returns:\n",
    "        list: List of tokenized words with stop words removed\"\"\"\n",
    "    \n",
    "    # list of stopwords\n",
    "    stop_words = ['a', 'an', 'the']\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the preprocessing pipeline to the text: `The quick brown fox jumps over the lazy dog.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = normalize(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = remove_stopwords(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quic', 'brow', 'fo', 'jump', 'ove', 'laz', 'do']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = stem(tokens)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the preprocessing techniques involved might vary depending on the task at hand and while in the example above functions were implemented from sctach, there are libraries that can help us with this task, for example **`NLTK`**, **`Spacy`**, **`Gensim`**, **`Scikit-learn`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we covered the basics of text preprocessing, we can introduce the concept of transforming this data to vector space. The result of this transformation is called a **`Document-Term Matrix`**. Since the number of zeros in this matrix is very high, it is usually also called **`Sparse Matrix`**.\n",
    "\n",
    "We will also the Bag of Words approach and make use of a few libraries since we already know how to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brow' 'do' 'fo' 'jump' 'laz' 'ove' 'quic']\n",
      "\n",
      "Vocabulary{'quic': 6, 'brow': 0, 'fo': 2, 'jump': 3, 'ove': 5, 'laz': 4, 'do': 1}\n",
      "\n",
      "Sparse Matrix:\n",
      "[[0 0 0 0 0 0 1]\n",
      " [1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0]\n",
      " [0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit_transform(tokens)\n",
    "\n",
    "# feature names\n",
    "print(vectorizer.get_feature_names_out(), end='\\n\\n')\n",
    "\n",
    "# vocabulary\n",
    "print(f'Vocabulary{vectorizer.vocabulary_}', end='\\n\\n')\n",
    "\n",
    "# print the vectorized sparse matrix\n",
    "print(f'Sparse Matrix:\\n{vectorizer.fit_transform(tokens).toarray()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result of the Bag of Words approach is a Document-Term Matrix.\n",
    "\n",
    "The rows of the matrix represent the documents, and the columns represent the words. The values of the matrix represent the frequency of the words in the documents.\n",
    "\n",
    "The problem with this approach is that it does not consider the order of the words, and it does not consider the context of the words. For example, the words “good” and “great” have similar meaning, but they are represented by different numbers. Also, the words “good” and “bad” have opposite meaning, but they are represented by similar numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings:\n",
    "----------------\n",
    "\n",
    "Word Embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
    "\n",
    "Word Embeddings are calculated by considering the context of the words. This means that words that have similar context will have similar representations.\n",
    "\n",
    "Word2Vec is one of the most popular techniques to obtain Word Embeddings. It is a two-layer neural network that processes text. Its input is a text corpus, and its output is a set of vectors: feature vectors for words in that corpus. While Word2Vec is not a deep neural network, it turns text into a numerical form that deep neural networks can understand.\n",
    "\n",
    "Word2Vec has two variants: Skip-Gram and CBOW.\n",
    "\n",
    "- Skip-Gram predicts the context given a word\n",
    "- Continuous Bag of Words (CBOW) predicts a word given the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "Tokens:\n",
      "['quic', 'brow', 'fo', 'jump', 'ove', 'laz', 'do']\n",
      "\n",
      "quic: [0.008132271468639374, -0.00445733405649662, -0.0010683572618290782, 0.0010063648223876953, -0.00019111395522486418, 0.001148177427239716, 0.006113860756158829, -2.0271540051908232e-05, -0.0032459653448313475, -0.0015107286162674427, 0.00589729892089963, 0.0015141022158786654, -0.0007242619758471847, 0.009333247318863869, -0.004921283572912216, -0.0008384096436202526, 0.00917541142553091, 0.0067494274117052555, 0.0015028560301288962, -0.008882560767233372, 0.0011487459996715188, -0.0022882556077092886, 0.009368237107992172, 0.0012099278392270207, 0.0014900636160746217, 0.002406409941613674, -0.0018360066460445523, -0.004999633878469467, 0.00023242950555868447, -0.002014180412515998, 0.006600933149456978, 0.00894012302160263, -0.0006747543811798096, 0.0029770147521048784, -0.006107654422521591, 0.00169932481367141, -0.006926232483237982, -0.008694026619195938, -0.005900202784687281, -0.00895647518336773, 0.0072775948792696, -0.005772031378000975, 0.008276351727545261, -0.007243545260280371, 0.0034216749481856823, 0.009674998931586742, -0.007785447873175144, -0.009945057332515717, -0.004329146351665258, -0.00268313055858016, -0.0002712893474381417, -0.008831551298499107, -0.00861755758523941, 0.0028002106118947268, -0.008206406608223915, -0.009069336578249931, -0.002340465784072876, -0.008631807751953602, -0.007056649774312973, -0.008401150815188885, -0.00030132889514788985, -0.004564298316836357, 0.006627174559980631, 0.001527160406112671, -0.003341475734487176, 0.006108971778303385, -0.006013284903019667, -0.004656169563531876, -0.007207509130239487, -0.004336580168455839, -0.0018093299586325884, 0.006489642895758152, -0.0027703929226845503, 0.0049189673736691475, 0.0069044423289597034, -0.007463705725967884, 0.004564850125461817, 0.006126978434622288, -0.002954474650323391, 0.006625021807849407, 0.0061258794739842415, -0.006443485151976347, -0.006764551624655724, 0.0025389587972313166, -0.0016238188836723566, -0.006065127905458212, 0.009499209001660347, -0.0051301466301083565, -0.006554096937179565, -0.00011988520418526605, -0.002701428020372987, 0.00044440029887482524, -0.003537458134815097, -0.0004193306085653603, -0.0007086157565936446, 0.0008228206424973905, 0.0081948172301054, -0.005736707244068384, -0.0016595280030742288, 0.0055716075003147125]\n",
      "brow: [-0.00872748252004385, 0.0021301615051925182, -0.0008735441952012479, -0.00931908842176199, -0.009428142569959164, -0.0014107179595157504, 0.004432408604770899, 0.003704071044921875, -0.006498693022876978, -0.00687306746840477, -0.004999412223696709, -0.002286844188347459, -0.0072502875700592995, -0.009603317826986313, -0.0027436292730271816, -0.008362840861082077, -0.006038875784724951, -0.0056709288619458675, -0.0023441375233232975, -0.0017069971654564142, -0.008956998586654663, -0.0007351994281634688, 0.008152506314218044, 0.0076904296875, -0.007206115871667862, -0.0036668311804533005, 0.0031185520347207785, -0.009570722468197346, 0.00147643918171525, 0.006524466443806887, 0.005746419541537762, -0.008763061836361885, -0.0045171440578997135, -0.008140160702168941, 4.59563743788749e-05, 0.009263633750379086, 0.005973305553197861, 0.005067307967692614, 0.005061062518507242, -0.003242917126044631, 0.00955218356102705, -0.0073564243502914906, -0.007270387373864651, -0.0022653890773653984, -0.0007785606430843472, -0.0032161034177988768, -0.0005925858276896179, 0.007488823030143976, -0.0006975185824558139, -0.001624940661713481, 0.002744399243965745, -0.008359100669622421, 0.007855803705751896, 0.00853610411286354, -0.009584086947143078, 0.002446266356855631, 0.009904971346259117, -0.0076658036559820175, -0.006966918706893921, -0.007736517116427422, 0.008395923301577568, -0.0006813359213992953, 0.009144408628344536, -0.008158220909535885, 0.0037430846132338047, 0.0026350426487624645, 0.0007427132222801447, 0.0023276759311556816, -0.007469093892723322, -0.00935837347060442, 0.0023545764852315187, 0.006148455198854208, 0.007985688745975494, 0.005735894665122032, -0.0007773363613523543, 0.008306164294481277, -0.009336314164102077, 0.003406132571399212, 0.0002667534281499684, 0.003857244271785021, 0.007385783363133669, -0.00672516692429781, 0.005584480706602335, -0.00952222477644682, -0.0008044588612392545, -0.008688736706972122, -0.005098673049360514, 0.009289226494729519, -0.0018582618795335293, 0.0029144263826310635, 0.00907127931714058, 0.008938132785260677, -0.008208435028791428, -0.0030123137403279543, 0.0098866056650877, 0.005104430951178074, -0.0015880870632827282, -0.008692021481692791, 0.0029615163803100586, -0.006675897631794214]\n",
      "fo: [-0.007139015011489391, 0.0012410306371748447, -0.007176716346293688, -0.002244618022814393, 0.003719303524121642, 0.0058331238105893135, 0.0011981832794845104, 0.002102731494233012, -0.00411039125174284, 0.007225333247333765, -0.006307041738182306, 0.004647215828299522, -0.008219973184168339, 0.002036467893049121, -0.004977052100002766, -0.004247688222676516, -0.00310898432508111, 0.005655208602547646, 0.005798400845378637, -0.00497464882209897, 0.0007733309175819159, -0.00849577784538269, 0.007809805683791637, 0.009257291443645954, -0.002742327516898513, 0.0008002233225852251, 0.0007466519018635154, 0.0054778847843408585, -0.008606079034507275, 0.0005844557308591902, 0.006869422271847725, 0.00223159440793097, 0.001124676433391869, -0.009322155267000198, 0.008482366800308228, -0.0062641273252666, -0.0029923736583441496, 0.003493787022307515, -0.0007726275944150984, 0.001411291304975748, 0.001781991682946682, -0.006828899495303631, -0.009724811650812626, 0.009040584787726402, 0.0061980546452105045, -0.006912927608937025, 0.0034034824930131435, 0.0002060639817500487, 0.004753745626658201, -0.007119942922145128, 0.004026954062283039, 0.004347434267401695, 0.009957369416952133, -0.004473739769309759, -0.0013892638962715864, -0.007317321375012398, -0.009697829373180866, -0.009080257266759872, -0.0010227549355477095, -0.00650329003110528, 0.00484972819685936, -0.0061640264466404915, 0.0025191856548190117, 0.0007394409039989114, -0.003392153885215521, -0.0009792232885956764, 0.00997912511229515, 0.00914588663727045, -0.004461829550564289, 0.009083027020096779, -0.005641763098537922, 0.005930921994149685, -0.0030972182285040617, 0.003431751625612378, 0.003017226466909051, 0.006900460924953222, -0.0023738837335258722, 0.008775036782026291, 0.007589428219944239, -0.009547646157443523, -0.008008209057152271, -0.007637896575033665, 0.002923257416114211, -0.0027947223279625177, -0.006929520517587662, -0.008128263987600803, 0.008309179916977882, 0.0019904887303709984, -0.009328017011284828, -0.004792716354131699, 0.0031367384362965822, -0.004713206086307764, 0.0052808430045843124, -0.004233441315591335, 0.002641795901581645, -0.008045687340199947, 0.00620988616719842, 0.00481888884678483, 0.0007871926063671708, 0.0030134476255625486]\n",
      "jump: [-0.00824267789721489, 0.009299354627728462, -0.00019766092009376734, -0.001967276446521282, 0.004603630397468805, -0.004095315933227539, 0.0027431142516434193, 0.006939966697245836, 0.006065425928682089, -0.007510794326663017, 0.009382350370287895, 0.004671808332204819, 0.003966120537370443, -0.006243505515158176, 0.00845997966825962, -0.002150164917111397, 0.00882518757134676, -0.005362002644687891, -0.008129418827593327, 0.006824559066444635, 0.0016711927019059658, -0.0021985089406371117, 0.00951360072940588, 0.009493854828178883, -0.009774046950042248, 0.0025052286218851805, 0.006156692281365395, 0.00387245649471879, 0.002022787230089307, 0.00043050170643255115, 0.000673631438985467, -0.003820636309683323, -0.007140250410884619, -0.0020888722501695156, 0.003923897631466389, 0.008818683214485645, 0.009259150363504887, -0.0059759365394711494, -0.00940267089754343, 0.009764377027750015, 0.0034297846723347902, 0.00516611710190773, 0.006282344926148653, -0.002804262563586235, 0.007322703488171101, 0.0028302716091275215, 0.0028710043989121914, -0.0023803699295967817, -0.003128249663859606, -0.0023701416794210672, 0.0042764367535710335, 7.605791324749589e-05, -0.009584278799593449, -0.009665544144809246, -0.006148193962872028, -0.00012856960529461503, 0.0019974159076809883, 0.00943196751177311, 0.005584350787103176, -0.004290696233510971, 0.00027831672923639417, 0.004964358638972044, 0.0076983096078038216, -0.0011442232644185424, 0.0043234205804765224, -0.005814379546791315, -0.0008041906403377652, 0.008100050501525402, -0.0023600650019943714, -0.009663455188274384, 0.00577926030382514, -0.003929822240024805, -0.0012228727573528886, 0.009980517439544201, -0.002256350591778755, -0.0047570643946528435, -0.00532938726246357, 0.006980889942497015, -0.005708871874958277, 0.0021136628929525614, -0.00525566004216671, 0.006120713893324137, 0.004357306752353907, 0.0026063548866659403, -0.0014910829486325383, -0.002746063517406583, 0.008992936462163925, 0.00521577475592494, -0.002162519609555602, -0.009470310062170029, -0.007426051888614893, -0.0010637413943186402, -0.0007949471473693848, -0.0025629091542214155, 0.00968272052705288, -0.00045852066250517964, 0.005873761139810085, -0.007447587326169014, -0.002506073797121644, -0.005549863446503878]\n",
      "ove: [9.456396219320595e-05, 0.003077319823205471, -0.006812645122408867, -0.0013754654210060835, 0.007668580859899521, 0.007346409372985363, -0.003673297120258212, 0.0026427018456161022, -0.0083171296864748, 0.006205486133694649, -0.004637322388589382, -0.0031641065143048763, 0.009311356581747532, 0.0008733856957405806, 0.0074907029047608376, -0.0060740625485777855, 0.0051605068147182465, 0.009922822937369347, -0.008457391522824764, -0.00513569125905633, -0.007064837031066418, -0.004862651694566011, -0.0037785638123750687, -0.008536199107766151, 0.00795560609549284, -0.004843938164412975, 0.008423613384366035, 0.005262570455670357, -0.006550026126205921, 0.003957871347665787, 0.00547014968469739, -0.007426536176353693, -0.007405719719827175, -0.002475230721756816, -0.008625725284218788, -0.001581572345457971, -0.0004034328449051827, 0.0032996844965964556, 0.0014418804785236716, -0.0008814215543679893, -0.005594057962298393, 0.001730365795083344, -0.0008973717922344804, 0.006793690845370293, 0.003973590210080147, 0.004529471509158611, 0.0014343059156090021, -0.0026998554822057486, -0.004366812761873007, -0.0010320746805518866, 0.0014370274730026722, -0.002646008739247918, -0.007073782850056887, -0.007805306930094957, -0.009121786803007126, -0.005935169290751219, -0.0018474245443940163, -0.004323871340602636, -0.006460670381784439, -0.0037173223681747913, 0.004289158619940281, -0.0037390433717519045, 0.008378175087273121, 0.0015339935198426247, -0.007242319639772177, 0.009433798491954803, 0.007631212472915649, 0.005493281874805689, -0.0068488456308841705, 0.005822679027915001, 0.0040090931579470634, 0.005185369402170181, 0.0042559015564620495, 0.0019397544674575329, -0.0031701624393463135, 0.008353845216333866, 0.009612180292606354, 0.0037926030345261097, -0.0028369950596243143, 7.127523531380575e-06, 0.0012188184773549438, -0.008458324708044529, -0.008223945274949074, -0.00023101568513084203, 0.0012372875353321433, -0.005743380635976791, -0.004725273698568344, -0.007346074562519789, 0.008328615687787533, 0.00012129783863201737, -0.004509398713707924, 0.005701705347746611, 0.009180014953017235, -0.004099871963262558, 0.007964681833982468, 0.0053754341788589954, 0.005879123229533434, 0.0005125903990119696, 0.008213084191083908, -0.0070190406404435635]\n",
      "laz: [-0.008619687519967556, 0.0036657380405813456, 0.0051898835226893425, 0.005741938482969999, 0.00746691832318902, -0.006167675368487835, 0.0011056136572733521, 0.006047282367944717, -0.002840050496160984, -0.006173522677272558, -0.00041022300138138235, -0.008368948474526405, -0.005600012373179197, 0.007104538846760988, 0.003352539613842964, 0.007225669454783201, 0.006800247356295586, 0.007530741859227419, -0.0037891543470323086, -0.0005618059658445418, 0.00234837643802166, -0.004519032314419746, 0.008388731628656387, -0.009858164004981518, 0.006764641031622887, 0.0029144168365746737, -0.004932831507176161, 0.00439818762242794, -0.0017395747127011418, 0.006711384281516075, 0.009964849799871445, -0.004362443462014198, -0.0005993377999402583, -0.0056956373155117035, 0.00385082233697176, 0.0027866268064826727, 0.006891076453030109, 0.006101095583289862, 0.009538496844470501, 0.009273417294025421, 0.007898068055510521, -0.006989504210650921, -0.00915586482733488, -0.0003557527088560164, -0.003099840832874179, 0.00789431668817997, 0.005938574206084013, -0.0015456628752872348, 0.0015109634259715676, 0.001790040754713118, 0.007817571051418781, -0.00951018650084734, -0.00020553111971821636, 0.0034691966138780117, -0.0009389722254127264, 0.008381771855056286, 0.009010783396661282, 0.006536506582051516, -0.0007116210181266069, 0.0077104042284190655, -0.008534334599971771, 0.0032071066088974476, -0.004637997131794691, -0.0050889551639556885, 0.0035896182525902987, 0.0053703393787145615, 0.007769514340907335, -0.005766506306827068, 0.007433360908180475, 0.006625496316701174, -0.0037098003085702658, -0.008745641447603703, 0.005437467247247696, 0.006509755738079548, -0.0007875502342358232, -0.006709855981171131, -0.007085925433784723, -0.0024970602244138718, 0.005143253598362207, -0.0036652374546974897, -0.009370059706270695, 0.0038267397321760654, 0.004884479101747274, -0.006428563501685858, 0.001208558096550405, -0.002074877033010125, 2.4403334464295767e-05, -0.009883509017527103, 0.00269200443290174, -0.004750106483697891, 0.0010876464657485485, -0.0015762245748192072, 0.002196673071011901, -0.00788157619535923, -0.002717183902859688, 0.0026631986256688833, 0.005346681922674179, -0.0023915148340165615, -0.009510094299912453, 0.004505878780037165]\n",
      "do: [-0.0005362272495403886, 0.0002364313550060615, 0.005103349685668945, 0.009009272791445255, -0.009302949532866478, -0.007116809021681547, 0.006458872463554144, 0.008972988463938236, -0.005015428178012371, -0.0037633716128766537, 0.007380504626780748, -0.0015334713971242309, -0.00453661335632205, 0.006554051768034697, -0.004860160406678915, -0.0018160176696255803, 0.002876579761505127, 0.000991873792372644, -0.008285215124487877, -0.009448817931115627, 0.0073117660358548164, 0.005070262122899294, 0.006757693365216255, 0.0007628655293956399, 0.006350890267640352, -0.0034053658600896597, -0.0009464013855904341, 0.005768573377281427, -0.007521637715399265, -0.003936103545129299, -0.007511582225561142, -0.0009300422389060259, 0.009538118727505207, -0.007319166790693998, -0.002333768643438816, -0.001937741064466536, 0.008077437058091164, -0.005930895917117596, 4.516244007390924e-05, -0.0047537339851260185, -0.009603550657629967, 0.005007293075323105, -0.008759585209190845, -0.004391825292259455, -3.5099983506370336e-05, -0.00029618144617415965, -0.007661240175366402, 0.009614743292331696, 0.004982057958841324, 0.00923314318060875, -0.00815791729837656, 0.004495798144489527, -0.00413707597181201, 0.0008245360804721713, 0.008498620241880417, -0.004462176468223333, 0.004517500288784504, -0.006786960177123547, -0.0035484887193888426, 0.0093985078856349, -0.0015776526415720582, 0.00032137156813405454, -0.0041406298987567425, -0.007682688068598509, -0.0015080082230269909, 0.002469794824719429, -0.0008880269597284496, 0.0055336616933345795, -0.002742977114394307, 0.002260065171867609, 0.005455794278532267, 0.008345953188836575, -0.001453740638680756, -0.009208142757415771, 0.004370552487671375, 0.0005717849708162248, 0.007441908121109009, -0.0008132827351801097, -0.002638413803651929, -0.008753009140491486, -0.0008565568714402616, 0.0028265630826354027, 0.005401428788900375, 0.007052656263113022, -0.0057031214237213135, 0.001858819741755724, 0.006088863592594862, -0.004798050969839096, -0.0031072604469954967, 0.0067976294085383415, 0.001631475635804236, 0.00018991708930116147, 0.0034736371599137783, 0.00021777748770546168, 0.00961882621049881, 0.005060603842139244, -0.008917390368878841, -0.007041560485959053, 0.0009014558745548129, 0.006392533890902996]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "print(f'Original text:\\n{text}\\n')\n",
    "\n",
    "print(f'Tokens:\\n{tokens}\\n')\n",
    "\n",
    "# Define and train Word2Vec model\n",
    "model_w2v = Word2Vec([tokens], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Get embeddings for each token\n",
    "word_embeddings = {word: model_w2v.wv[word].tolist() for word in tokens}\n",
    "\n",
    "# Print the word embeddings\n",
    "for word, embedding in word_embeddings.items():\n",
    "    print(f\"{word}: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Vectors (GloVe)\n",
    "----------------------\n",
    "\n",
    "GloVe is an unsupervised learning algorithm for obtaining word embeddings. GloVe is similar to Word2Vec, but it is different in the way it is trained.\n",
    "\n",
    "GloVe is trained on the non-zero elements of the word-word co-occurrence matrix, which is a matrix that contains how frequently words co-occur with each other.\n",
    "\n",
    "The result of GloVe is a matrix that contains the word embeddings for each word. The rows of the matrix represent the words, and the columns represent the dimensions of the embeddings.\n",
    "\n",
    "The dimensions of the embeddings are usually between 50 and 300. The number of dimensions is a hyperparameter that can be tuned.\n",
    "\n",
    "Then, it is important to highlight that:\n",
    "\n",
    "- We can obtain the word embeddings by using the GloVe library which contains a dictionary with the word embeddings for the most common words in the English language.\n",
    "\n",
    "- We can train our own word embeddings by using the GloVe library since it contains a function that allows us to train our own word embeddings using a corpus of text.\n",
    "\n",
    "Link to the library: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Link to the paper: https://nlp.stanford.edu/pubs/glove.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "-----------\n",
    "\n",
    "- [Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze, Introduction to Information Retrieval, Cambridge University Press. 2008.](https://nlp.stanford.edu/IR-book/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
